import numpy as npfrom matplotlib import pyplot as pltimport pandas as pdimport plotly.express as pximport seaborn as snsfrom sklearn.ensemble import IsolationForestfrom src.eda.eda_misc import plot_data_distributionfrom src.utils.my_dataframe import custom_infodef eda(gcr):    """    Performs exploratory data analysis (EDA) on the provided dataset to prepare it for more effective modeling and insights derivation.    Objectives and Techniques:    - Features sanity, types, and values: Evaluate if the features contain nan values, and attempt to distinguish continuous from discrete ones.        Discrete features could be natively numerical or arising from encoding of boolean or categorical variables.        Use df.info() and df.nunique() to check the number of nans, the feature type and the number of unique values each feature assumes.    - Distribution and Outliers: Evaluate the distribution of features and the target. The aim is assessing the data is well-conditioned for learning (scale, statistical assumptions, no outliers).        Use histograms, boxplots, violinplots, countplots, and shape statistics to assess distribution.        Use Tukey thresholding or isolation forests for univariate and multivariate outlier detection.    - Feature-Target Relationships: Characterize relationships between features and the target in terms of strength and nature to inform feature engineering and model selection.        Perform a quick assessment, use pairplots and feature ranking based on mutual information. The pairplot, or specific scatter plots, is also important for the next steps.        To assess linearity/monotonicity in regression, use the Pearson and Spearman correlation coefficients for continuous features or grouped boxplots for discrete ones.        To assess homoscedasticity in regression, check for fan-shaped patterns in the pairplot (homoscedasticity involves feat-residuals, but feat-target can be a model-agnostic indication).        To assess linear class separability in binary classification (or one-hot encoded target class), use statistical feature selection based on ANOVA f-score if the feature is continuous or chi2 if the feature is discrete.        To assess linear class separability in multiclass classification (label-encoded target class), just check the pairplot.    - Feature Interactions: Investigate relationships between features to inform feature engineering or gain insights into the underlying processes.        To assess feature redundancy, check if the pearson corrcoef or the cramer V approach 1 for continuous-continuous or discrete-discrete relationships, respectively.        To evaluate feature combination or gain insights on underlying process, check feature pairs that have high spearman corrcoef or cramer V for continuous-continuous or discrete-discrete relationships, respectively.        To identify and explain complex phenomena, check for abrupt trend changes in the scatterplot.    - Existing Clusters: Use scatterplots and dimensionality reduction techniques like PCA or t-SNE to visualize and assess data separability for classification and expected number of clusters.    Args:        gcr (DataFrame): A pandas DataFrame containing the data to be analyzed.    Returns:        Outputs various visualizations that illustrate the data's characteristics and summaries of statistical tests to guide further data preprocessing and feature engineering decisions.    """    # region Feature Sanity    # Distinguish continuous from discrete columns    print(f"Number of nans, type, and number of unique values for each column of the DataFrame: \n{custom_info(gcr)}")    # Discrimination between continuous and discrete features is based on a heuristic thresholding, derived manually from the analysis of the name, type, and unique value of each feature.    cols_continuous = gcr.nunique().loc[gcr.nunique() > 8].index.to_list()    cols_discrete = gcr.nunique().loc[gcr.nunique() <= 8].index.to_list()    # endregion    # region Distribution and Outliers    print("Plot distribution of continuous and of discrete variables.")    fig_cont, fig_disc = plot_data_distribution(gcr)    print("Identify outliers using Random Isolation Forest")    outliers = IsolationForest(n_estimators=100, random_state=0).fit(gcr[cols_continuous]).predict(gcr[cols_continuous]) == -1    print("Number of identified outliers:", outliers.sum())    # endregion    # region Feature-Target Relationships    # endregion    pass