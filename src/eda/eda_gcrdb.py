import numpy as npfrom matplotlib import pyplot as pltimport pandas as pdimport plotly.express as pximport seaborn as snsfrom sklearn.ensemble import IsolationForestfrom sklearn.feature_selection import SelectKBest, mutual_info_classiffrom src.eda.eda_misc import plot_data_distribution, plot_feature_clusters, plot_pairwise_scatterplots, \    compute_mutual_information, compute_relationshipfrom src.utils.my_dataframe import custom_infodef eda(gcr):    """    Performs exploratory data analysis (EDA) on the provided dataset to prepare it for more effective modeling and insights derivation.    Objectives and Techniques:    - Features sanity, types, and values: Evaluate if the features contain nan values, and attempt to distinguish continuous from discrete ones.        Discrete features could be natively numerical or arising from encoding of boolean or categorical variables.        Use df.info() and df.nunique() to check the number of nans, the feature type and the number of unique values each feature assumes.    - Distribution and Outliers: Evaluate the distribution of features and the target. The aim is assessing the data is well-conditioned for learning (scale, statistical assumptions, no outliers).        Use histograms, boxplots, violinplots, countplots, and shape statistics to assess distribution.        Use Tukey thresholding or isolation forests for univariate and multivariate outlier detection.    - Existing Clusters: Use scatterplots, optionally preceeded by dimensionality reduction techniques like PCA or t-SNE, to visualize and assess data separability for classification and expected number of clusters.    - Feature-Target Relationships: Characterize relationships between features and the target in terms of strength and nature to inform feature engineering and model selection.        Perform a quick assessment, use pairplots and feature ranking based on mutual information. The pairplot, or specific scatter plots, is also important for the next steps.        To assess linearity/monotonicity in regression, use the Pearson and Spearman correlation coefficients for continuous features or grouped boxplots for discrete ones.        To assess homoscedasticity in regression, check for fan-shaped patterns in the pairplot (homoscedasticity involves feat-residuals, but feat-target can be a model-agnostic indication).        To assess linear class separability in binary classification (or one-hot encoded target class), use statistical feature selection based on ANOVA f-score if the feature is continuous or chi2 if the feature is discrete.        To assess linear class separability in multiclass classification (label-encoded target class), just check the pairplot.    - Feature Interactions: Investigate relationships between features to inform feature engineering or gain insights into the underlying processes.        To assess feature redundancy, check if the pearson corrcoef or the cramer V approach 1 for continuous-continuous or discrete-discrete relationships, respectively.        To evaluate feature combination or gain insights on underlying process, check feature pairs that have high spearman corrcoef or cramer V for continuous-continuous or discrete-discrete relationships, respectively.        To identify and explain complex phenomena, check for abrupt trend changes in the scatterplot.    Args:        gcr (DataFrame): A pandas DataFrame containing the data to be analyzed.    Returns:        Outputs various visualizations that illustrate the data's characteristics and summaries of statistical tests to guide further data preprocessing and feature engineering decisions.    """    # region Feature Sanity    # Distinguish continuous from discrete columns    print(f"Number of nans, type, and number of unique values for each column of the DataFrame: \n{custom_info(gcr)}")    # Here, the discrimination between continuous and discrete features is based on a heuristic thresholding, derived manually from the analysis of the name, type, and unique value of each feature.    cols_continuous = gcr.nunique().loc[gcr.nunique() > 8].index.to_list()    cols_discrete = gcr.nunique().loc[gcr.nunique() <= 8].index.to_list()    # endregion    # region Distribution and Outliers    print("Plot distribution of continuous and of discrete variables.")    fig_cont, fig_disc = plot_data_distribution(gcr)    print("Identify outliers using Random Isolation Forest.")    outliers = IsolationForest(n_estimators=100, random_state=0).fit(gcr[cols_continuous]).predict(        gcr[cols_continuous]) == -1    print("Number of identified outliers:", outliers.sum())    # endregion    # region Cluster features    print("Plot a 2D projections of the continuous features to identify natural clusters.")    fig_cont_clusters = plot_feature_clusters(gcr, columns_to_plot=cols_continuous, color_labels=gcr["Good Risk"])    # endregion    # region Feature-Target Relationships    print("Pairplot features vs target variable")    fig_pairplots = plot_pairwise_scatterplots(gcr, target_columns=["Good Risk"], color_labels=gcr["Good Risk"],                                               sample_size=100)    print("General relationships features vs target variable.\n"          "Mutual information is used to explore nonlinear relationships between any type of variable (discrete or continuous).\n"          "The correct function calls (involving mutual_info_classif or mutual_info_regression, as well as the argument discrete_features) are handled automatically.")    (results_cont,     results_disc,     fig_heatmap_mi_cont,     fig_heatmap_mi_disc) = compute_mutual_information(gcr,                                                       columns_of_interest=gcr.columns,                                                       target="Good Risk",                                                       discrete_features_mask=[col in cols_discrete for col in gcr.columns],                                                       plot_heatmap=True)    # TODO: Check if in this case f-statistics or cramer v are most suited (discrete target, mixed features).    print("Linear relationships features vs target variable ('Good Risk')\n"          "Using f_classif to explore both continuous-discrete and discrete-discrete linear relationships.")  # Sklearn's documentation refers to the F-statistics as linear ones, although I couldn't find a proper further support evidence. https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection:~:text=The%20methods%20based%20on%20F%2Dtest%20estimate%20the%20degree%20of%20linear%20dependency%20between%20two%20random%20variables.    (linear_relationship_feature_target,     fig_heatmap_relationship) = compute_relationship(gcr,                                                      score_func="f_classif",  # Alternatives might have been pearson for continuous-continuous relationships                                                      columns_of_interest=gcr.columns,                                                      target="Good Risk",                                                      sample_size=1000,                                                      plot_heatmap=True,                                                      include_diagonal=True,  # This avoids plotting relationships of a variable with itself                                                      )    print(f"Linear relationship between features and target:\n{linear_relationship_feature_target}")    # print("Monotonic relationships features vs target variable ('Good Risk')\n"    #       "Using spearman coefficient to explore both continuous-discrete and discrete-discrete linear relationships.")  # Sklearn's documentation refers to the F-statistics as linear ones, although I couldn't find a proper further support evidence. https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection:~:text=The%20methods%20based%20on%20F%2Dtest%20estimate%20the%20degree%20of%20linear%20dependency%20between%20two%20random%20variables.    # (linear_relationship_feature_target,    #  fig_heatmap_relationship) = compute_relationship(gcr,    #                                                   score_func="spearman",    #                                                   # Alternatives might have been pearson for continuous-continuous relationships    #                                                   columns_of_interest=gcr.columns,    #                                                   target="Good Risk",    #                                                   sample_size=1000,    #                                                   plot_heatmap=True,    #                                                   include_diagonal=True,    #                                                   # This avoids plotting relationships of a variable with itself    #                                                   )    # print(f"Monotonic relationship between features and target:\n{linear_relationship_feature_target}")    # endregion    # region Feature-Feature Relationships    print("Pairplot between features")    fig_pairplots = plot_pairwise_scatterplots(gcr, columns_to_plot=[col for col in gcr.columns.to_list() if                                                                     col != "Good Risk"], color_labels=gcr["Good Risk"],                                               color_interpretation="Good Risk", sample_size=100)    # endregion    pass