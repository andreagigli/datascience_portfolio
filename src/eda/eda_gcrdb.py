from pandas import DataFramefrom sklearn.ensemble import IsolationForestfrom src.eda.eda_misc import plot_data_distribution, plot_clusters_2d, plot_pairwise_scatterplots, compute_mutual_information, compute_relationshipfrom src.utils.my_dataframe import custom_infodef eda(gcr: DataFrame) -> None:    """    Performs exploratory data analysis (EDA) on the provided dataset to prepare it for more effective modeling and insights derivation.    Objectives and Techniques:    - Features sanity, types, and values: Evaluate if the features contain nan values, and attempt to distinguish continuous from discrete ones.        Discrete features could be natively numerical or arising from encoding of boolean or categorical variables.        Use df.info() and df.nunique() to check the number of nans, the feature type and the number of unique values each feature assumes.    - Distribution and Outliers: Evaluate the distribution of features and the target. The aim is assessing the data is well-conditioned for learning (scale, statistical assumptions, no outliers).        Use histograms, boxplots, violinplots, countplots, and shape statistics to assess distribution.        Use Tukey thresholding or isolation forests for univariate and multivariate outlier detection.    - Existing Clusters: Use scatterplots, optionally preceeded by dimensionality reduction techniques like PCA or t-SNE, to visualize and assess data separability for classification and expected number of clusters.    - Feature-Target Relationships: Characterize relationships between features and the target in terms of strength and nature to inform feature engineering and model selection.        Perform a quick assessment, use pairplots and feature ranking based on mutual information. The pairplot, or specific scatter plots, is also important for the next steps.        To assess linearity/monotonicity in regression, use the Pearson and Spearman correlation coefficients for continuous features or grouped boxplots for discrete ones.        To assess homoscedasticity in regression, check for fan-shaped patterns in the pairplot (homoscedasticity involves feat-residuals, but feat-target can be a model-agnostic indication).        To assess linear class separability in binary classification (or one-hot encoded target class), use statistical feature selection based on ANOVA f-score if the feature is continuous or chi2 if the feature is discrete.        To assess linear class separability in multiclass classification (label-encoded target class), just check the pairplot.    - Feature Interactions: Investigate relationships between features to inform feature engineering or gain insights into the underlying processes.        To assess feature redundancy, check if the pearson corrcoef or the cramer V approach 1 for continuous-continuous or discrete-discrete relationships, respectively.        To evaluate feature combination or gain insights on underlying process, check feature pairs that have high spearman corrcoef or cramer V for continuous-continuous or discrete-discrete relationships, respectively.        To identify and explain complex phenomena, check for abrupt trend changes in the scatterplot.    Args:        gcr (DataFrame): A pandas DataFrame containing the data to be analyzed.    Returns:        Outputs various visualizations that illustrate the data's characteristics and summaries of statistical tests to guide further data preprocessing and feature engineering decisions.    """    # region Feature Sanity    # Distinguish continuous from discrete columns    print(f"Number of nans, type, and number of unique values for each column of the DataFrame: \n{custom_info(gcr)}")    # Here, the discrimination between continuous and discrete features is based on a heuristic thresholding, derived manually from the analysis of the name, type, and unique value of each feature.    cols_continuous = gcr.nunique().loc[gcr.nunique() > 8].index.to_list()    cols_discrete = gcr.nunique().loc[gcr.nunique() <= 8].index.to_list()    # endregion    # region Distribution and Outliers    print("Plot distribution of continuous and of discrete variables.")    (fig_distribution_continuous,     fig_distribution_discrete) = plot_data_distribution(gcr, discrete_features_mask=cols_discrete)    print("Identify outliers using Random Isolation Forest.")    isofor = IsolationForest(n_estimators=100, random_state=0)    isofor.fit(gcr[cols_continuous])    outliers = isofor.predict(gcr[cols_continuous]) == -1    print("Number of identified outliers:", outliers.sum())    # endregion    # region Cluster features    print("Plot a 2D projections of the continuous features to identify natural clusters.")    fig_cont_clusters = plot_clusters_2d(gcr, columns_to_plot=cols_continuous, color_labels=gcr["Good Risk"])    # endregion    # region Feature-Target Relationships    print("Pairplot of features vs target ('Good Risk').\nUseful to inform feature engineering and model selection.\n")    fig_pairplot_feature_target = plot_pairwise_scatterplots(gcr,                                                             target_columns=["Good Risk"],                                                             color_labels=gcr["Good Risk"],                                                             sample_size=100)    print("General relationships between features and target ('Good Risk').\n"          "Mutual information is used to explore nonlinear relationships between the target and all features, regardless of their discrete or continuous type.\n"          "Useful to inform feature engineering and model selection.\n")    # The correct mutual information criterion (mutual_info_classif or mutual_info_regression depending on the target type)    # and 'discrete_features' argument (depending on the feature type) are handled automatically    (mi_feature_continuoustarget, mi_feature_discretetarget,     fig_heatmap_mi_feature_continuoustarget, fig_heatmap_mi_feature_discretetarget) = compute_mutual_information(        gcr,        columns_of_interest=gcr.columns,        target="Good Risk",        discrete_features_mask=[col in cols_discrete for col in gcr.columns],        plot_heatmap=True,        include_diagonal=False,    )    # # For regression tasks, one can check the existence of linear or monotonic feature-target relationships. This is not the case in this analysis.    # print("Linear relationships between features and target ('...').\n"    #       "Useful to inform selection of regression models.")    # linear_feature_target, fig_heatmap_linear_feature_target = compute_relationship(    #     gcr,    #     score_func="pearson",    #     columns_of_interest=gcr.columns,    #     target="Good Risk",    #     sample_size=1000,    #     plot_heatmap=True,    #     include_diagonal=True,  # This avoids plotting relationships of a variable with itself    #     )    #    # print("Monotonic relationships between features and target ('...').\n"    #       "Useful to inform the adoption of simple linearizing features.")    # monotonic_feature_target, fig_heatmap_monotonic_feature_target = compute_relationship(    #     gcr,    #     score_func="spearman",    #     columns_of_interest=gcr.columns,    #     target="Good Risk",    #     sample_size=1000,    #     plot_heatmap=True,    #     include_diagonal=True,  # This avoids plotting relationships of a variable with itself    #     )    # endregion    # region Feature-Feature Relationships    print("Pairplot of feature vs feature.\n"          "Useful to explain underlying processes and spot edge conditions, inform the feature elimination or feature joining.")    fig_pairplot_feature_feature = plot_pairwise_scatterplots(        gcr,        columns_to_plot=[col for col in gcr.columns.to_list() if col != "Good Risk"],        color_labels=gcr["Good Risk"],  # For classification tasks, use this column to color the samples        color_interpretation="Good Risk",  # For classification tasks, use this column to color the samples        sample_size=100,    )    print("General relationships between pairs of features.\n"          "Computed with mutual information and reported separately for continuous 'target features' and for discrete 'target features'.\n "          "Useful to explain underlying processes.")    (mi_feature_continuousfeature, mi_feature_discretefeature,     fig_heatmap_mi_feature_continuousfeature, fig_heatmap_mi_feature_discretefeature) = compute_mutual_information(        gcr,        columns_of_interest=gcr.columns,        discrete_features_mask=[col in cols_discrete for col in gcr.columns],        plot_heatmap=True,        include_diagonal=False,    )    print("Linear relationships between pairs of continuous features.\nUseful to inform feature elimination.")    linear_feature_feature, fig_heatmap_linear_feature_feature = compute_relationship(        gcr,        score_func="pearson",        columns_of_interest=cols_continuous,        sample_size=1000,        plot_heatmap=True,        include_diagonal=True,  # This avoids plotting relationships of a variable with itself    )    print("Monotonic relationships between pairs of continuous features.\nUseful to inform feature combination.")    monotonic_feature_feature, fig_heatmap_monotonic_feature_feature = compute_relationship(        gcr,        score_func="spearman",        columns_of_interest=cols_continuous,        sample_size=1000,        plot_heatmap=True,        include_diagonal=True,  # This avoids plotting relationships of a variable with itself    )    # endregion